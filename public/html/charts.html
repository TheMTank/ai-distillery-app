{% extends "layouts/post_page_layout.html" %}

{% block content %}
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.7.2/Chart.bundle.js"></script>

    <title>All Charts</title>
    <style>
        .chart-heading {
            text-align: center;
            margin-bottom: 40px;
            margin-top: 40px;
        }

        #ai-distillery-title {
            text-align: center;
        }
    </style>


<div id="content-wrapper">
      <div class="inner clearfix">
          <!--todo have a header to jump between charts -->
        <section id="main-content">
          <h1 id="ai-distillery-title">AI Distillery charts</h1>
            <div>
                <h1 class="chart-heading">Number of papers released on arxiv over time</h1>
                <canvas id="myChart" width="1000" height="400"></canvas>

                <h1 class="chart-heading">The number of mentions of "GAN" in titles over time since 2014</h1>
                <canvas id="GANChart" width="1000" height="400"></canvas>

                <h1 class="chart-heading">Most cited papers according to semantic scholar from 2014+ in our arxiv set</h1>
                <canvas id="most-cited-papers" width="1000" height="1000"></canvas>

                <h1 class="chart-heading">Top cited papers over time</h1>
                <canvas id="top-cited-papers-over-time" width="600" height="500"></canvas>

                <h1 class="chart-heading">Authors who have published the most papers from 2014+</h1>
                <canvas id="top-n-authors" width="1000" height="1000"></canvas>
            </div>

        </section>
      </div>
</div>
{% endblock %}

{% block javascript %}
{{ super() }}
<script>
$( document ).ready(function() {
    var ctx = document.getElementById("myChart").getContext('2d');
    /* TODO: have vertical line underneath closest point like in google charts but also indicates you can hover */

    /*new Chart(document.getElementById("chartjs-0"),{"type":"line","data":{"labels":["January","February","March","April","May","June","July"],
    "datasets":[{"label":"My First Dataset","data":[65,59,80,81,56,55,40],"fill":false,"borderColor":"rgb(75, 192, 192)","lineTension":0.1}]},"options":{}});*/

    var ctx = document.getElementById("myChart").getContext('2d');
      var myLineChart = new Chart(ctx, {
        type: 'line',
        data: {
        labels: ['14-1', '14-2', '14-3', '14-4', '14-5', '14-6', '14-7', '14-8', '14-9', '14-10', '14-11', '14-12', '15-1', '15-2', '15-3', '15-4', '15-5', '15-6', '15-7', '15-8', '15-9', '15-10', '15-11', '15-12', '16-1', '16-2', '16-3', '16-4', '16-5', '16-6', '16-7', '16-8', '16-9', '16-10', '16-11', '16-12', '17-1', '17-2', '17-3', '17-4', '17-5', '17-6', '17-7', '17-8', '17-9', '17-10', '17-11', '17-12', '18-1', '18-2', '18-3', '18-4', '18-5', '18-6', '18-7', '18-8'],

          datasets: [ {
                    label: 'Num papers released on arxiv over time (cs.[CV|CL|LG|AI|NE] / stat.ML)',
                    data: [211, 255, 214, 228, 270, 270, 241, 217, 223, 245, 299, 263, 241, 296, 287, 303, 324, 351, 302, 279, 343, 306, 399, 342, 285, 362, 431, 469, 492, 578, 498, 493, 530, 581, 621, 557, 443, 533, 674, 699, 773, 729, 858, 849, 862, 725, 992, 818, 719, 1041, 1169, 1243, 1698, 1600, 1577, 1418],
                    fill:false,
                    borderColor:"rgb(75, 192, 192)",
                    "lineTension":0.1}]
        },
        options: {responsive: false}
    });

    var GANctx = document.getElementById("GANChart").getContext('2d');
      var ganChart = new Chart(GANctx, {
        type: 'line',
        data: {
        labels: ['14-1', '14-2', '14-3', '14-4', '14-5', '14-6', '14-7', '14-8', '14-9', '14-10', '14-11', '14-12', '15-1', '15-2', '15-3', '15-4', '15-5', '15-6', '15-7', '15-8', '15-9', '15-10', '15-11', '15-12', '16-1', '16-2', '16-3', '16-4', '16-5', '16-6', '16-7', '16-8', '16-9', '16-10', '16-11', '16-12', '17-1', '17-2', '17-3', '17-4', '17-5', '17-6', '17-7', '17-8', '17-9', '17-10', '17-11', '17-12', '18-1', '18-2', '18-3', '18-4', '18-5', '18-6', '18-7', '18-8', '18-9'],

        datasets: [ {
                    label: 'Num GAN papers released on arxiv over time (cs.[CV|CL|LG|AI|NE] / stat.ML)',
                    data: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 10, 6, 3, 3, 3, 4, 11, 9, 13, 9, 8, 6, 26, 25, 14, 28, 23, 24, 35, 35, 31, 21, 29],
                    fill:false,
                    borderColor:"rgb(75, 192, 192)",
                    "lineTension":0.1}]
        },
        options: {responsive: false}
    });

    var top_cited_paper_labels_top_100 = ['Explaining and Harnessing Adversarial Examples', 'Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs', 'Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation', 'Microsoft COCO: Common Objects in Context', 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks', 'SSD: Single Shot MultiBox Detector', 'Convolutional Neural Networks for Sentence Classification', 'Show and tell: A neural image caption generator', 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift', 'Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification', 'Return of the Devil in the Details: Delving Deep into Convolutional Nets', 'FaceNet: A unified embedding for face recognition and clustering', 'OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks', 'Neural Machine Translation by Jointly Learning to Align and Translate', 'Deep Residual Learning for Image Recognition', 'Scikit-learn: Machine Learning in Python', 'Image-to-Image Translation with Conditional Adversarial Networks', 'TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems', 'Going deeper with convolutions', 'MatConvNet - Convolutional Neural Networks for MATLAB', 'Deep Learning in Neural Networks: An Overview', 'TensorFlow: A system for large-scale machine learning', 'Long-Term Recurrent Convolutional Networks for Visual Recognition and Description', 'Rethinking the Inception Architecture for Computer Vision', 'Densely Connected Convolutional Networks', 'Intriguing properties of neural networks', 'Caffe: Convolutional Architecture for Fast Feature Embedding', 'ImageNet Large Scale Visual Recognition Challenge', 'Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks', 'Improved Techniques for Training GANs', 'Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition', 'Sequence to Sequence Learning with Neural Networks', 'Representation Learning: A Review and New Perspectives', 'Very Deep Convolutional Networks for Large-Scale Image Recognition', 'Deep Visual-Semantic Alignments for Generating Image Descriptions', 'Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation', 'Distributed Representations of Sentences and Documents', 'You Only Look Once: Unified, Real-Time Object Detection', 'Fast R-CNN', 'Show, Attend and Tell: Neural Image Caption Generation with Visual Attention', 'Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling', 'Adam: A Method for Stochastic Optimization', 'Effective Approaches to Attention-based Neural Machine Translation', 'Two-Stream Convolutional Networks for Action Recognition in Videos', 'CNN Features Off-the-Shelf: An Astounding Baseline for Recognition', 'Fully Convolutional Networks for Semantic Segmentation', 'Identity Mappings in Deep Residual Networks', 'How transferable are features in deep neural networks?', 'Fully Convolutional Networks for Semantic Segmentation', 'A Convolutional Neural Network for Modelling Sentences', 'Generating Sequences With Recurrent Neural Networks', 'Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding', 'DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs', 'Distilling the Knowledge in a Neural Network', 'Learning Spatiotemporal Features with 3D Convolutional Networks', 'High-Speed Tracking with Kernelized Correlation Filters', 'Network In Network', 'Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning', 'Multi-Scale Context Aggregation by Dilated Convolutions', 'Conditional Random Fields as Recurrent Neural Networks', 'On the Properties of Neural Machine Translation: Encoder-Decoder Approaches', 'Asynchronous Methods for Deep Reinforcement Learning', 'Spatial Transformer Networks', 'SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation', 'ORB-SLAM: A Versatile and Accurate Monocular SLAM System', 'Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps', 'Learning Deconvolution Network for Semantic Segmentation', 'Conditional Generative Adversarial Nets', 'DeepWalk: Online Learning of Social Representations', 'Continuous control with deep reinforcement learning', 'Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network', 'Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks', 'Perceptual Losses for Real-Time Style Transfer and Super-Resolution', 'Deep Learning Face Attributes in the Wild', 'XGBoost: A Scalable Tree Boosting System', 'Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks', 'Recurrent Neural Network Regularization', 'Theano: A Python framework for fast computation of mathematical expressions', 'GraphLab: A New Framework For Parallel Machine Learning', 'VQA: Visual Question Answering', 'Image Super-Resolution Using Deep Convolutional Networks', 'Neural Machine Translation of Rare Words with Subword Units', 'The Cityscapes Dataset for Semantic Urban Scene Understanding', 'Striving for Simplicity: The All Convolutional Net', "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", 'Learning both Weights and Connections for Efficient Neural Networks', 'YOLO9000: Better, Faster, Stronger', 'node2vec: Scalable Feature Learning for Networks', 'Deep neural networks are easily fooled: High confidence predictions for unrecognizable images', 'Beyond short snippets: Deep networks for video classification', 'DRAW: A Recurrent Neural Network For Image Generation', 'Attention Is All You Need', 'Teaching Machines to Read and Comprehend', 'Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks', 'DeepPose: Human Pose Estimation via Deep Neural Networks', 'Recurrent Models of Visual Attention', 'Deep Learning Face Representation by Joint Identification-Verification', 'Character-level Convolutional Networks for Text Classification', 'Improved Training of Wasserstein GANs', 'WaveNet: A Generative Model for Raw Audio'];
    var top_cited_paper_citation_counts_top_100 = [999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 995, 984, 959, 939, 923, 917, 899, 897, 876, 844, 841, 831, 826, 824, 813, 812, 810, 810, 770, 769, 761, 753, 753, 748, 731, 730, 719, 714, 711, 679, 679, 675, 674, 658, 645, 638, 632, 628, 620, 616, 614, 612, 612, 608, 602, 602, 599, 599, 592, 592, 591];

    background_colours = []
    for (var i = 0; i < top_cited_paper_labels_top_100.length; i++) {
        background_colours.push("rgba(" + (255 - i * 5) + ", " + (i * 2) + ", 0, 0.5");
    }

    new Chart(document.getElementById("most-cited-papers"),
        {"type":"horizontalBar",
         "data": {"labels":top_cited_paper_labels_top_100,
                  "datasets":[
                        {"label":"Most cited papers according to semantic scholar from 2014+ in our arxiv set",
                         "data":top_cited_paper_citation_counts_top_100,
                         "fill":false,
                         "backgroundColor": background_colours,
                         "borderWidth":1,
                         "hoverBackgroundColor": "rgb(35, 20, 207)"}
                         ]
                   },
         "options":{"scales":{"xAxes":[{"ticks":{"beginAtZero":true}}]}}});


    var top_n_author_labels = ['Yoshua Bengio', 'Chunhua Shen', 'Sergey Levine', 'Luc Van Gool', 'Pieter Abbeel', 'Trevor Darrell', 'Liang Lin', 'Kyunghyun Cho', 'Shuicheng Yan', 'Eric P. Xing', 'Jiashi Feng', 'Yang Liu', 'Xiaogang Wang', 'Alan L. Yuille', 'Bernt Schiele', 'Yi Yang', 'Nassir Navab', 'Dacheng Tao', 'Ruslan Salakhutdinov', 'Lei Zhang', 'Anton van den Hengel', 'Ming-Hsuan Yang', 'Aaron C. Courville', 'Shie Mannor', 'Philip H. S. Torr', 'Wei Liu', 'Michael I. Jordan', 'Jitendra Malik', 'Jun Zhu', 'Xiaodan Liang', 'Daniel Cremers', 'Abhinav Gupta', 'Li Fei-Fei', 'Max Welling', 'Mario Fritz', 'Xiaoou Tang', 'Andrea Vedaldi', 'Chris Dyer', 'Larry S. Davis', 'Bernhard Schölkopf', 'Oriol Vinyals', 'Jianfeng Gao', 'Yu Zhang', 'Rama Chellappa', 'Bo Li', 'Andrew Zisserman', 'Lawrence Carin', 'Graham Neubig', 'Dhruv Batra', 'Barnabás Póczos', 'Devi Parikh', 'Yann LeCun', 'Cordelia Schmid', 'Ian D. Reid', 'Le Song', 'Chen Change Loy', 'Thomas S. Huang', 'Daniel Rueckert', 'Silvio Savarese', 'Wei Li', 'Xi Chen', 'Xiaodong He', 'Joshua B. Tenenbaum', 'Tong Zhang', 'Ming Zhou', 'Jun Wang', 'Wei Wang', 'Alfred O. Hero', 'Svetha Venkatesh', 'Song-Chun Zhu', 'Mathieu Salzmann', 'Gang Wang', 'Philip S. Yu', 'Percy Liang', 'Pascal Fua', 'Andreas Krause', 'Pushmeet Kohli', 'Xu Sun', 'Alexander Wong', 'Timothy M. Hospedales', 'Tao Xiang', 'Hinrich Schütze', 'Kate Saenko', 'Anima Anandkumar', 'Nathan Srebro', 'Peng Wang', 'Andreas K. Maier', 'Antonio Torralba', 'Masashi Sugiyama', 'Noah A. Smith', 'Ian J. Goodfellow', 'Wangmeng Zuo', 'Stefano Ermon', 'Quoc V. Le', 'Ross B. Girshick', 'Jan Kautz', 'Tie-Yan Liu', 'Weinan Zhang', 'Honglak Lee', 'Jian Sun'];
    var top_n_author_counts = [201, 126, 119, 114, 101, 96, 96, 93, 91, 88, 87, 87, 87, 86, 83, 81, 76, 76, 74, 73, 73, 71, 69, 67, 67, 67, 66, 66, 65, 65, 65, 64, 64, 62, 62, 62, 62, 62, 61, 60, 60, 60, 59, 59, 58, 58, 57, 57, 56, 56, 54, 54, 54, 53, 53, 53, 53, 52, 52, 51, 51, 51, 50, 50, 50, 50, 50, 50, 49, 49, 49, 49, 49, 49, 49, 49, 49, 48, 48, 48, 48, 48, 48, 47, 47, 47, 47, 47, 47, 47, 46, 46, 46, 45, 45, 45, 45, 45, 45, 44];

    new Chart(document.getElementById("top-n-authors"),
        {"type":"horizontalBar",
         "data": {"labels":top_n_author_labels,
                  "datasets":[
                        {"label":"Authors who have published the most papers from 2014+",
                         "data":top_n_author_counts,
                         "fill":false,
                         "backgroundColor": background_colours,
                         "borderWidth":1,
                         "hoverBackgroundColor": "rgb(35, 20, 207)"}
                         ]
                   },
         "options":{"scales":{"xAxes":[{"ticks":{"beginAtZero":true}}]}}});

    var top_cited_papers_over_time = [{'x': 2014, 'y': 999, 'r': 24}, {'x': 2014, 'y': 999, 'r': 24}, {'x': 2014, 'y': 999, 'r': 24}, {'x': 2014, 'y': 999, 'r': 24}, {'x': 2015, 'y': 999, 'r': 24}, {'x': 2016, 'y': 999, 'r': 24}, {'x': 2014, 'y': 999, 'r': 24}, {'x': 2015, 'y': 999, 'r': 24}, {'x': 2015, 'y': 999, 'r': 24}, {'x': 2015, 'y': 999, 'r': 24}, {'x': 2014, 'y': 999, 'r': 24}, {'x': 2015, 'y': 999, 'r': 24}, {'x': 2014, 'y': 999, 'r': 24}, {'x': 2016, 'y': 999, 'r': 24}, {'x': 2017, 'y': 999, 'r': 24}, {'x': 2015, 'y': 999, 'r': 24}, {'x': 2015, 'y': 999, 'r': 24}, {'x': 2015, 'y': 999, 'r': 24}, {'x': 2015, 'y': 999, 'r': 24}, {'x': 2016, 'y': 999, 'r': 24}, {'x': 2015, 'y': 999, 'r': 24}, {'x': 2016, 'y': 999, 'r': 24}, {'x': 2017, 'y': 999, 'r': 24}, {'x': 2014, 'y': 999, 'r': 24}, {'x': 2015, 'y': 999, 'r': 24}, {'x': 2015, 'y': 999, 'r': 24}, {'x': 2016, 'y': 999, 'r': 24}, {'x': 2014, 'y': 999, 'r': 24}, {'x': 2014, 'y': 999, 'r': 24}, {'x': 2014, 'y': 999, 'r': 24}, {'x': 2015, 'y': 999, 'r': 24}, {'x': 2014, 'y': 999, 'r': 24}, {'x': 2014, 'y': 999, 'r': 24}, {'x': 2016, 'y': 999, 'r': 24}, {'x': 2015, 'y': 999, 'r': 24}, {'x': 2015, 'y': 999, 'r': 24}, {'x': 2014, 'y': 999, 'r': 24}, {'x': 2014, 'y': 999, 'r': 24}, {'x': 2015, 'y': 999, 'r': 24}, {'x': 2014, 'y': 999, 'r': 24}, {'x': 2014, 'y': 999, 'r': 24}, {'x': 2015, 'y': 999, 'r': 24}, {'x': 2016, 'y': 999, 'r': 24}, {'x': 2014, 'y': 999, 'r': 24}, {'x': 2015, 'y': 999, 'r': 24}, {'x': 2014, 'y': 995, 'r': 24}, {'x': 2015, 'y': 959, 'r': 23}, {'x': 2018, 'y': 939, 'r': 23}, {'x': 2015, 'y': 923, 'r': 23}, {'x': 2015, 'y': 917, 'r': 22}, {'x': 2015, 'y': 899, 'r': 22}, {'x': 2017, 'y': 876, 'r': 21}, {'x': 2015, 'y': 844, 'r': 21}, {'x': 2015, 'y': 841, 'r': 21}, {'x': 2014, 'y': 831, 'r': 20}, {'x': 2016, 'y': 826, 'r': 20}, {'x': 2015, 'y': 824, 'r': 20}, {'x': 2017, 'y': 813, 'r': 20}, {'x': 2015, 'y': 812, 'r': 20}, {'x': 2015, 'y': 810, 'r': 20}, {'x': 2014, 'y': 770, 'r': 19}, {'x': 2014, 'y': 769, 'r': 19}, {'x': 2015, 'y': 761, 'r': 19}, {'x': 2017, 'y': 753, 'r': 18}, {'x': 2017, 'y': 753, 'r': 18}, {'x': 2016, 'y': 748, 'r': 18}, {'x': 2015, 'y': 731, 'r': 18}, {'x': 2016, 'y': 730, 'r': 18}, {'x': 2015, 'y': 719, 'r': 17}, {'x': 2014, 'y': 714, 'r': 17}, {'x': 2016, 'y': 711, 'r': 17}, {'x': 2015, 'y': 679, 'r': 16}, {'x': 2016, 'y': 675, 'r': 16}, {'x': 2016, 'y': 674, 'r': 16}, {'x': 2016, 'y': 658, 'r': 16}, {'x': 2014, 'y': 645, 'r': 16}, {'x': 2016, 'y': 638, 'r': 15}, {'x': 2015, 'y': 632, 'r': 15}, {'x': 2017, 'y': 628, 'r': 15}, {'x': 2016, 'y': 620, 'r': 15}, {'x': 2015, 'y': 616, 'r': 15}, {'x': 2015, 'y': 614, 'r': 15}, {'x': 2015, 'y': 612, 'r': 15}, {'x': 2017, 'y': 612, 'r': 15}, {'x': 2015, 'y': 608, 'r': 15}, {'x': 2015, 'y': 602, 'r': 15}, {'x': 2014, 'y': 602, 'r': 15}, {'x': 2014, 'y': 599, 'r': 14}, {'x': 2014, 'y': 599, 'r': 14}, {'x': 2015, 'y': 592, 'r': 14}, {'x': 2017, 'y': 592, 'r': 14}, {'x': 2016, 'y': 591, 'r': 14}];
    //var top_cited_papers_over_time = [{'x': 'Explaining and Harnessing Adversarial Examples', 'y': 999, 'r': 24}, {'x': 'Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs', 'y': 999, 'r': 24}, {'x': 'Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation', 'y': 999, 'r': 24}, {'x': 'Microsoft COCO: Common Objects in Context', 'y': 999, 'r': 24}, {'x': 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks', 'y': 999, 'r': 24}, {'x': 'SSD: Single Shot MultiBox Detector', 'y': 999, 'r': 24}, {'x': 'Convolutional Neural Networks for Sentence Classification', 'y': 999, 'r': 24}, {'x': 'Show and tell: A neural image caption generator', 'y': 999, 'r': 24}, {'x': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift', 'y': 999, 'r': 24}, {'x': 'Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification', 'y': 999, 'r': 24}, {'x': 'Return of the Devil in the Details: Delving Deep into Convolutional Nets', 'y': 999, 'r': 24}, {'x': 'FaceNet: A unified embedding for face recognition and clustering', 'y': 999, 'r': 24}, {'x': 'Neural Machine Translation by Jointly Learning to Align and Translate', 'y': 999, 'r': 24}, {'x': 'Deep Residual Learning for Image Recognition', 'y': 999, 'r': 24}, {'x': 'Image-to-Image Translation with Conditional Adversarial Networks', 'y': 999, 'r': 24}, {'x': 'TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems', 'y': 999, 'r': 24}, {'x': 'Going deeper with convolutions', 'y': 999, 'r': 24}, {'x': 'MatConvNet - Convolutional Neural Networks for MATLAB', 'y': 999, 'r': 24}, {'x': 'Deep Learning in Neural Networks: An Overview', 'y': 999, 'r': 24}, {'x': 'TensorFlow: A system for large-scale machine learning', 'y': 999, 'r': 24}, {'x': 'Long-Term Recurrent Convolutional Networks for Visual Recognition and Description', 'y': 999, 'r': 24}, {'x': 'Rethinking the Inception Architecture for Computer Vision', 'y': 999, 'r': 24}, {'x': 'Densely Connected Convolutional Networks', 'y': 999, 'r': 24}, {'x': 'Caffe: Convolutional Architecture for Fast Feature Embedding', 'y': 999, 'r': 24}, {'x': 'ImageNet Large Scale Visual Recognition Challenge', 'y': 999, 'r': 24}, {'x': 'Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks', 'y': 999, 'r': 24}, {'x': 'Improved Techniques for Training GANs', 'y': 999, 'r': 24}, {'x': 'Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition', 'y': 999, 'r': 24}, {'x': 'Sequence to Sequence Learning with Neural Networks', 'y': 999, 'r': 24}, {'x': 'Very Deep Convolutional Networks for Large-Scale Image Recognition', 'y': 999, 'r': 24}, {'x': 'Deep Visual-Semantic Alignments for Generating Image Descriptions', 'y': 999, 'r': 24}, {'x': 'Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation', 'y': 999, 'r': 24}, {'x': 'Distributed Representations of Sentences and Documents', 'y': 999, 'r': 24}, {'x': 'You Only Look Once: Unified, Real-Time Object Detection', 'y': 999, 'r': 24}, {'x': 'Fast R-CNN', 'y': 999, 'r': 24}, {'x': 'Show, Attend and Tell: Neural Image Caption Generation with Visual Attention', 'y': 999, 'r': 24}, {'x': 'Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling', 'y': 999, 'r': 24}, {'x': 'Adam: A Method for Stochastic Optimization', 'y': 999, 'r': 24}, {'x': 'Effective Approaches to Attention-based Neural Machine Translation', 'y': 999, 'r': 24}, {'x': 'Two-Stream Convolutional Networks for Action Recognition in Videos', 'y': 999, 'r': 24}, {'x': 'CNN Features Off-the-Shelf: An Astounding Baseline for Recognition', 'y': 999, 'r': 24}, {'x': 'Fully Convolutional Networks for Semantic Segmentation', 'y': 999, 'r': 24}, {'x': 'Identity Mappings in Deep Residual Networks', 'y': 999, 'r': 24}, {'x': 'How transferable are features in deep neural networks?', 'y': 999, 'r': 24}, {'x': 'Fully Convolutional Networks for Semantic Segmentation', 'y': 999, 'r': 24}, {'x': 'A Convolutional Neural Network for Modelling Sentences', 'y': 995, 'r': 24}, {'x': 'Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding', 'y': 959, 'r': 23}, {'x': 'DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs', 'y': 939, 'r': 23}, {'x': 'Distilling the Knowledge in a Neural Network', 'y': 923, 'r': 23}, {'x': 'Learning Spatiotemporal Features with 3D Convolutional Networks', 'y': 917, 'r': 22}, {'x': 'High-Speed Tracking with Kernelized Correlation Filters', 'y': 899, 'r': 22}, {'x': 'Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning', 'y': 876, 'r': 21}, {'x': 'Multi-Scale Context Aggregation by Dilated Convolutions', 'y': 844, 'r': 21}, {'x': 'Conditional Random Fields as Recurrent Neural Networks', 'y': 841, 'r': 21}, {'x': 'On the Properties of Neural Machine Translation: Encoder-Decoder Approaches', 'y': 831, 'r': 20}, {'x': 'Asynchronous Methods for Deep Reinforcement Learning', 'y': 826, 'r': 20}, {'x': 'Spatial Transformer Networks', 'y': 824, 'r': 20}, {'x': 'SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation', 'y': 813, 'r': 20}, {'x': 'ORB-SLAM: A Versatile and Accurate Monocular SLAM System', 'y': 812, 'r': 20}, {'x': 'Learning Deconvolution Network for Semantic Segmentation', 'y': 810, 'r': 20}, {'x': 'Conditional Generative Adversarial Nets', 'y': 770, 'r': 19}, {'x': 'DeepWalk: Online Learning of Social Representations', 'y': 769, 'r': 19}, {'x': 'Continuous control with deep reinforcement learning', 'y': 761, 'r': 19}, {'x': 'Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network', 'y': 753, 'r': 18}, {'x': 'Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks', 'y': 753, 'r': 18}, {'x': 'Perceptual Losses for Real-Time Style Transfer and Super-Resolution', 'y': 748, 'r': 18}, {'x': 'Deep Learning Face Attributes in the Wild', 'y': 731, 'r': 18}, {'x': 'XGBoost: A Scalable Tree Boosting System', 'y': 730, 'r': 18}, {'x': 'Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks', 'y': 719, 'r': 17}, {'x': 'Recurrent Neural Network Regularization', 'y': 714, 'r': 17}, {'x': 'Theano: A Python framework for fast computation of mathematical expressions', 'y': 711, 'r': 17}, {'x': 'VQA: Visual Question Answering', 'y': 679, 'r': 16}, {'x': 'Image Super-Resolution Using Deep Convolutional Networks', 'y': 675, 'r': 16}, {'x': 'Neural Machine Translation of Rare Words with Subword Units', 'y': 674, 'r': 16}, {'x': 'The Cityscapes Dataset for Semantic Urban Scene Understanding', 'y': 658, 'r': 16}, {'x': 'Striving for Simplicity: The All Convolutional Net', 'y': 645, 'r': 16}, {'x': "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", 'y': 638, 'r': 15}, {'x': 'Learning both Weights and Connections for Efficient Neural Networks', 'y': 632, 'r': 15}, {'x': 'YOLO9000: Better, Faster, Stronger', 'y': 628, 'r': 15}, {'x': 'node2vec: Scalable Feature Learning for Networks', 'y': 620, 'r': 15}, {'x': 'Deep neural networks are easily fooled: High confidence predictions for unrecognizable images', 'y': 616, 'r': 15}, {'x': 'Beyond short snippets: Deep networks for video classification', 'y': 614, 'r': 15}, {'x': 'DRAW: A Recurrent Neural Network For Image Generation', 'y': 612, 'r': 15}, {'x': 'Attention Is All You Need', 'y': 612, 'r': 15}, {'x': 'Teaching Machines to Read and Comprehend', 'y': 608, 'r': 15}, {'x': 'Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks', 'y': 602, 'r': 15}, {'x': 'DeepPose: Human Pose Estimation via Deep Neural Networks', 'y': 602, 'r': 15}, {'x': 'Recurrent Models of Visual Attention', 'y': 599, 'r': 14}, {'x': 'Deep Learning Face Representation by Joint Identification-Verification', 'y': 599, 'r': 14}, {'x': 'Character-level Convolutional Networks for Text Classification', 'y': 592, 'r': 14}, {'x': 'Improved Training of Wasserstein GANs', 'y': 592, 'r': 14}, {'x': 'WaveNet: A Generative Model for Raw Audio', 'y': 591, 'r': 14}];
    var top_cited_papers_over_time_labels = ['Explaining and Harnessing Adversarial Examples', 'Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs', 'Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation', 'Microsoft COCO: Common Objects in Context', 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks', 'SSD: Single Shot MultiBox Detector', 'Convolutional Neural Networks for Sentence Classification', 'Show and tell: A neural image caption generator', 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift', 'Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification', 'Return of the Devil in the Details: Delving Deep into Convolutional Nets', 'FaceNet: A unified embedding for face recognition and clustering', 'Neural Machine Translation by Jointly Learning to Align and Translate', 'Deep Residual Learning for Image Recognition', 'Image-to-Image Translation with Conditional Adversarial Networks', 'TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems', 'Going deeper with convolutions', 'MatConvNet - Convolutional Neural Networks for MATLAB', 'Deep Learning in Neural Networks: An Overview', 'TensorFlow: A system for large-scale machine learning', 'Long-Term Recurrent Convolutional Networks for Visual Recognition and Description', 'Rethinking the Inception Architecture for Computer Vision', 'Densely Connected Convolutional Networks', 'Caffe: Convolutional Architecture for Fast Feature Embedding', 'ImageNet Large Scale Visual Recognition Challenge', 'Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks', 'Improved Techniques for Training GANs', 'Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition', 'Sequence to Sequence Learning with Neural Networks', 'Very Deep Convolutional Networks for Large-Scale Image Recognition', 'Deep Visual-Semantic Alignments for Generating Image Descriptions', 'Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation', 'Distributed Representations of Sentences and Documents', 'You Only Look Once: Unified, Real-Time Object Detection', 'Fast R-CNN', 'Show, Attend and Tell: Neural Image Caption Generation with Visual Attention', 'Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling', 'Adam: A Method for Stochastic Optimization', 'Effective Approaches to Attention-based Neural Machine Translation', 'Two-Stream Convolutional Networks for Action Recognition in Videos', 'CNN Features Off-the-Shelf: An Astounding Baseline for Recognition', 'Fully Convolutional Networks for Semantic Segmentation', 'Identity Mappings in Deep Residual Networks', 'How transferable are features in deep neural networks?', 'Fully Convolutional Networks for Semantic Segmentation', 'A Convolutional Neural Network for Modelling Sentences', 'Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding', 'DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs', 'Distilling the Knowledge in a Neural Network', 'Learning Spatiotemporal Features with 3D Convolutional Networks', 'High-Speed Tracking with Kernelized Correlation Filters', 'Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning', 'Multi-Scale Context Aggregation by Dilated Convolutions', 'Conditional Random Fields as Recurrent Neural Networks', 'On the Properties of Neural Machine Translation: Encoder-Decoder Approaches', 'Asynchronous Methods for Deep Reinforcement Learning', 'Spatial Transformer Networks', 'SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation', 'ORB-SLAM: A Versatile and Accurate Monocular SLAM System', 'Learning Deconvolution Network for Semantic Segmentation', 'Conditional Generative Adversarial Nets', 'DeepWalk: Online Learning of Social Representations', 'Continuous control with deep reinforcement learning', 'Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network', 'Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks', 'Perceptual Losses for Real-Time Style Transfer and Super-Resolution', 'Deep Learning Face Attributes in the Wild', 'XGBoost: A Scalable Tree Boosting System', 'Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks', 'Recurrent Neural Network Regularization', 'Theano: A Python framework for fast computation of mathematical expressions', 'VQA: Visual Question Answering', 'Image Super-Resolution Using Deep Convolutional Networks', 'Neural Machine Translation of Rare Words with Subword Units', 'The Cityscapes Dataset for Semantic Urban Scene Understanding', 'Striving for Simplicity: The All Convolutional Net', "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", 'Learning both Weights and Connections for Efficient Neural Networks', 'YOLO9000: Better, Faster, Stronger', 'node2vec: Scalable Feature Learning for Networks', 'Deep neural networks are easily fooled: High confidence predictions for unrecognizable images', 'Beyond short snippets: Deep networks for video classification', 'DRAW: A Recurrent Neural Network For Image Generation', 'Attention Is All You Need', 'Teaching Machines to Read and Comprehend', 'Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks', 'DeepPose: Human Pose Estimation via Deep Neural Networks', 'Recurrent Models of Visual Attention', 'Deep Learning Face Representation by Joint Identification-Verification', 'Character-level Convolutional Networks for Text Classification', 'Improved Training of Wasserstein GANs', 'WaveNet: A Generative Model for Raw Audio'];
    var num_citations = [999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 995, 959, 939, 923, 917, 899, 876, 844, 841, 831, 826, 824, 813, 812, 810, 770, 769, 761, 753, 753, 748, 731, 730, 719, 714, 711, 679, 675, 674, 658, 645, 638, 632, 628, 620, 616, 614, 612, 612, 608, 602, 602, 599, 599, 592, 592, 591];

    new Chart(document.getElementById("top-cited-papers-over-time"),
        {"type":"bubble",
         "data": {"labels":top_cited_papers_over_time_labels,
                  "datasets":[
                        {"label":"Top cited papers over time from 2014+",
                         "data":top_cited_papers_over_time,
                         "fill":false,
                         "backgroundColor": background_colours,
                         "borderWidth":1,
                         "hoverBackgroundColor": "rgb(35, 20, 207)"}
                         ]
                   },
         "options": {"scales":{"xAxes":[{"ticks":{"stepSize": 1, "beginAtZero":false}}]},
                     "tooltips": {
                      callbacks: {
                        title: function(tooltipItem, data) {
                          return top_cited_papers_over_time_labels[tooltipItem[0]['index']];
                        },
                        label: function(tooltipItem, data) {
                          return 'Num Citations: ' + tooltipItem['yLabel'];
                        }
                      }
                    }
                    }});

});
</script>
{% endblock %}
